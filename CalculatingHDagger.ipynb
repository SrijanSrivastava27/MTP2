{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6ml1pL1-4Amc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_gaussian_blur(img, sigma, kernel_size):\n",
        "    device = img.device  # Store the original device\n",
        "    img = img.squeeze(0).detach().cpu().numpy() #SHape is now 28x28\n",
        "    img = cv2.GaussianBlur(img, (kernel_size, kernel_size), sigmaX=sigma, sigmaY=sigma, borderType= cv2.BORDER_CONSTANT)\n",
        "    img = torch.tensor(img, dtype=torch.float32).unsqueeze(0).to(device)  # Restore shape with correct device\n",
        "    img.requires_grad = True\n",
        "    return img"
      ],
      "metadata": {
        "id": "WkXAj_cY4C5i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigma1 = 1\n",
        "kernel_size = 3\n",
        "x = torch.zeros(kernel_size, kernel_size)\n",
        "x[int(kernel_size/2), int(kernel_size/2)] = 1\n",
        "x = x.type(torch.float32)\n",
        "A_sigma1 = apply_gaussian_blur(x, sigma1, kernel_size)"
      ],
      "metadata": {
        "id": "31kgovvy4FmE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_sigma1 = A_sigma1.squeeze(0)"
      ],
      "metadata": {
        "id": "QVJUwi7p6LEL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_height = 28\n",
        "img_width = 28\n",
        "x = np.zeros((img_height, img_width))\n",
        "total_dim = x.ndim\n",
        "\n",
        "flat_size = 1\n",
        "for i in x.shape:\n",
        "  flat_size = flat_size * i\n",
        "\n",
        "H = np.zeros((flat_size, flat_size))\n",
        "\n"
      ],
      "metadata": {
        "id": "nF9caJYA-yjd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for r in range(flat_size):\n",
        "  for i in range(kernel_size):\n",
        "    for j in range(kernel_size):\n",
        "      n = r % img_height\n",
        "      m = r // img_height\n",
        "\n",
        "      if m - kernel_size//2 + i < 0 or m - kernel_size//2 + i >= img_height:\n",
        "        continue\n",
        "      if n - kernel_size//2 + j < 0 or n - kernel_size//2 + j >= img_width:\n",
        "        continue\n",
        "\n",
        "      H[(int)(m * img_width + n)\n",
        "        ][(int)((m - kernel_size//2 + i) * img_width + (n - kernel_size//2 + j))\n",
        "        ] = A_sigma1[i][j]"
      ],
      "metadata": {
        "id": "L1obmVmhU8wW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_clean = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = MNIST(root=\"./data\", train=True, transform=transform_clean, download=True)\n",
        "test_dataset = MNIST(root=\"./data\", train=False, transform=transform_clean, download=True)\n",
        "\n",
        "fraction = 1\n",
        "num_samples = len(train_dataset)\n",
        "indices = torch.arange(num_samples)[:int(fraction * num_samples)]\n",
        "\n",
        "sampler = SubsetRandomSampler(indices)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, sampler = sampler)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a86j6UqWQIP",
        "outputId": "07df55cd-c694-48eb-d25b-8a04b0e29445"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 478kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.43MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.97MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for clean_images, _ in train_loader:\n",
        "  print(clean_images.shape)\n",
        "  img = clean_images[0][0]\n",
        "  y1 = apply_gaussian_blur(img, sigma=sigma1, kernel_size = kernel_size)\n",
        "  y2 = H @ img.flatten().numpy()\n",
        "  y2 = y2.reshape(28,28)\n",
        "  y1 = y1[0].detach().numpy()\n",
        "  ans = y1-y2\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlhNK91TWStB",
        "outputId": "ac424472-c86d-47df-af67-939beddb08bb"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.linalg.pinv(torch.tensor(H)) @ torch.tensor(H)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmG4r0LCeKe7",
        "outputId": "006a29dc-6d75-4180-88c5-94958dca8d42"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.0000e+00,  3.1444e-14, -2.1917e-14,  ...,  8.8818e-16,\n",
              "         -2.1760e-14, -1.0880e-14],\n",
              "        [ 7.9737e-16,  1.0000e+00,  7.8388e-15,  ..., -4.6629e-15,\n",
              "          1.3101e-14,  5.1070e-15],\n",
              "        [-1.8619e-15,  7.3525e-15,  1.0000e+00,  ...,  5.2180e-15,\n",
              "         -5.6986e-16, -2.9057e-16],\n",
              "        ...,\n",
              "        [-2.3520e-15, -6.9388e-14,  1.0899e-13,  ...,  1.0000e+00,\n",
              "          7.2831e-14,  2.4869e-14],\n",
              "        [ 2.2674e-15,  5.4556e-14, -9.8697e-14,  ..., -1.9540e-14,\n",
              "          1.0000e+00, -1.2434e-14],\n",
              "        [-1.3303e-15, -3.5337e-14,  6.1706e-14,  ...,  7.1054e-15,\n",
              "          3.6415e-14,  1.0000e+00]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    }
  ]
}